[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "(**advanced, *proficient)\n\n\n\n\nR and R Studio**\nPython*\nQualtrics**\nHTML/CSS*\nDatabases/SQL*\nJavaScript*\nGitHub*\nSPSS*\nNVivo*\n\n\n\n\n\nExperimental design; AB Testing\nSurvey design\nData cleaning; manipulation\nData visualization; dashboards\nStatistical modeling\nMachine learning techniques\nCausal inference; hypothesis testing"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "",
    "text": "(**advanced, *proficient)\n\n\n\n\nR and R Studio**\nPython*\nQualtrics**\nHTML/CSS*\nDatabases/SQL*\nJavaScript*\nGitHub*\nSPSS*\nNVivo*\n\n\n\n\n\nExperimental design; AB Testing\nSurvey design\nData cleaning; manipulation\nData visualization; dashboards\nStatistical modeling\nMachine learning techniques\nCausal inference; hypothesis testing"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\nGraduate Student Researcher, Learning Research and Development Center (2020 - Present)\n\nUniversity of Pittsburgh\n\nExperimental Study: Mindfulness Training to Increase Learning and Engagement in Introductory Physics (Project overview on OSF)\nData: observational survey data from ~3000 students; Repeated-measures experimental data from ~450 students; Qualitative focus group interviews from 24 students\nResponsibilities\n\nData collection and management (participant interaction, quality assurance, organization, reporting)\nDesign implementation (experimental stimuli, online surveys, logic and randomization, programming)\nData analysis (coding, psychometrics, generalized linear modeling, mixed effects modeling, visualization)\nMentoring and team management\n\nKey contributions:\n\nRedesigned data collection procedures and survey design to increase accuracy and efficiency\nWrote custom code expand range of behavioral measures collected\n\n\n\n\n\nIndependent Research Contractor, Macmillan Learning (2023-2024)\n\nResponsibilities: Conducted in-person and virtual qualitative classroom observations"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nPhD, University of Pittsburgh, Cognitive Psychology (Expected 2026)\n\nMinor in Quantitative Methodology\n\n\n\nMS, University of Pittsburgh, Cognitive Psychology (2023)\n\n\nBS, Montana State University, Psychology (2020)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Avital Pelakh",
    "section": "",
    "text": "E-mail\n  \n  \n    \n     Resume\n  \n  \n    \n     CV\n  \n  \n    \n     LinkedIn\n  \n\n  \n  \nI am currently completing a Ph.D. in Cognitive Psychology at the University of Pittsburgh’s Learning Research and Development Center with a minor in Quantitative Methodology. My research interests are focused on reasoning, problem solving, and motivation in the context of STEM learning.\nI have a passion for working with data and possess strong skills in statistical analysis, data visualization, and survey programming and design. I also have experience with qualitative and mixed methods research.\nIn addition to my experience with research and analysis, I have a background in media production and graphic design which makes me especially well suited for creating effective science communication."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "College ScoreCard Explorer\n\n\n\n\n\nA Shiny app that allows the user to explore a large data set compiled by the U.S. Department of Education.\n\n\n\n\n\nJan 17, 2024\n\n\nAvital Pelakh\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Experiments\n\n\n\n\n\nAn interactive lesson on designing and interpreting scientific experiments. This lesson was created as part of a course at Carnegie Melon University using Qualtrics.\n\n\n\n\n\nJan 17, 2024\n\n\nAvital Pelakh\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics Problem Categorization\n\n\n\n\n\nFinal project for Data Science course at Carnegie Mellon in Spring 2022.\n\n\n\n\n\nMay 17, 2022\n\n\nAvital Pelakh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/physics_categorization/index.html",
    "href": "projects/physics_categorization/index.html",
    "title": "Physics Problem Categorization",
    "section": "",
    "text": "About\nThe following report was originally created in Jupyter for a course called Data Science for Psychology and Neuroscience, taken at Carnegie Mellon University in 2022. The analysis includes a brief background, description of data cleaning procedures, and hypothesis tests. Mixed-effects modeling, logistic analysis, and Bayesian methods are used.\n\n\n1. Project Title\nExamining Associations Between Stress Appraisals, Problem Categorization, and Solution Times in Undergraduate Physics Students\n\n\n2. Background\nPrior work has shown that experts tend to categorize physics problems based on the underlying principles used to solve them (deep structure), while novices tend to rely more on surface features, or the superficial features of a problem (Chi, Feltovich, and Glaser, 1981). While it was initially thought that expert-novice differences were due to “poorly formed, qualitatively different, or nonexistent categories in the novice representation”, follow-up work showed that strategy—in addition to knowledge structure—contributes to novice performance. Compared to other novices, those who attempt to categorize problems by underlying principles tend to be better problem solvers, even when they are no better at doing so correctly and controlling for math ability (Hardiman, Dufresne, and Mestre, 1989). This begs the question: what predicts strategy selection in novices?\nHere we propose that solution time may be an important predictor of strategy selection. In a related study by Heckler and Scaife (2011) using simpler physics items, researchers found that incorrect responses were faster when a “misconception-like” response option was available. They further found that imposing a three second delay improved performance and that students answering incorrectly could identify the correct rule when explicitly asked to do so. The researchers observed that students appeared to know the correct answer (or the correct rule), but answered incorrectly due to a motivation to answer quickly, and likened the phenomenon to a sort of Stroop effect. There is some suggestion that this effect may generalize to categorization judgements. For example, Chi et al. (1981) reported that experts spent more time categorizing problems despite having a fluency advantage, and Hardiman et al. (1989) suggested a similarity threshold explanation of novice responding, but to our knowledge, no study has looked at the association between solution time and categorization judgments.\nWe further propose that students experiencing psychological threat will have greater motivation to answer quickly. We define psychological threat in terms of the biopsychosocial model of challenge and threat (Blascovich, 2008; Blascovich & Tomaka, 1996; Jamieson et al., 2016;  Seery, 2011, 2013), which proposes that students experience threat (as opposed to challenge) when perceptions of situational demands exceed perceptions of coping resources in a motivated performance context. According to the theory, threatened students will be more inclined to avoid the task rather than approach or engage, so they should be more motivated to answer quickly.\n\n2.1 Research Questions\n\n1. What is the association between solution time, surface feature distractors, and accurate problem categorization in physics?\n2. Is greater psychological threat associated with quicker categorization judgments when surface feature distractors are present?\n\n\n\n2.2 Current Work\nThe current study seeks to address these questions in a sample of undergraduate physics students who reported experiencing psychological threat on a screening survey and were enrolled in a week-long research study. During the baseline assessment, participants completed a series of self-report surveys, including an 11-item measure of psychological threat. Afterward, they completed a three-part, 10-item physics assessment. The second part of the assessment was a five-item problem categorization task similar to that used in Hardiman et al. (1989). Participants completed all measures remotely using Qualtrics survey delivery platform. Here we report on analyses conducted on this baseline data (before random assignment).\n\n\n\n3. Variables\n\n3.1 Metadata and Identification\n\nID: Participant ID\n\nSemester: Semester that the participant was enrolled in the study\n\nWeek: Week of the semester that the student participated (Week 1 = first week of the semester, Week 15 = final week of the semester).\n\nPhysAssessVersion: Which version of the physics assessment they recieved. There were two equivalent versions balanced between baseline and posttest.\n\nQuestion: Physics assessment item number (2-6)\n\n\n\n3.2 Survey Measures and Demographics\n\nPsych.Threat.Diff: Psychological threat was measured using self report survey following Jamieson et al., 2016. There were 11 items and two subscales: resources (5 items) and demands (6 items). Participants rated their agreement to a series of statements such as “Working on physics is very stressful” or “I view physics as a positive challenge” on a scale from 1-6. The subscales were calculated by taking the mean ratings, and psychological threat was calculated by taking the difference between demands and resources such that a score greater than zero represents psychological threat and a score less than zero represents psychological challenge.\nGender: Self-reported gender identity. Because there were only 2 participants who self-reported as non-binary, female and non-binary identifying individuals were combined. This is because physics courses, including the ones we recruited from, are generally male-dominated.\n\n\n\n3.3 Categorization Task Measures\n\nResponse: Raw response selection (alternative 1 or alternative 2)\n\nScore: 0 = correct, 1 = incorrect\n\nAccuracy: factor version of score\n\nTime: Time until page submit, measured in seconds\n\nDistractors: Present = surface features in the incorrect response option match the model problem, Absent = surface features in the incorrect response option are unrelated to the model\n\n\nFigure 1. Sample of a Categorization Task Item with Distractors\n\n\n\nCategorization Task\n\n\n\nNote. Adapted from Hardiman et al. (1989) and Docktor et al. (2015). Five categorization task items were given as a part of a series of physics tasks during the baseline session of the experiment (before the intervention). There were two equivalent versions of the assessment (A and B). Four items included an incorrect response option that only superficially matched the model (distractors-present), and there was one item where surface features were unrelated to the model. Participants were given two minutes to solve each problem, but could not proceed without making a selection. When the timer was up, they would recieve a warning on their screen to make a selection before moving on. Therefore, it was possible to have a solution time longer than two minutes.\n\n\n\n\n4. Hypotheses\nH1: There will be a significant interaction between item type (distractors or not), solution time, and accuracy. Specifically, we expect that when distractors are present, increases in solution time will be associated with higher likelihood of solving the problem correctly\n$Y_{Accuracy} = (*0 + b{ID,0,id} + b_{QUESTION,0,question}) + $ $*{Distractors} + _{Version} + $ \\(\\beta_{Semester} + \\beta_{Week} + \\beta_{Gender} + \\epsilon\\)\nH2: Psychological threat, measured using self-report survey, will be negatively associated with time spent on the problem categorization task items when distractors are present.\n$Y_{Time} = (*0 + b{ID,0,id} + b_{QUESTION,0,question}) + $ $*{Distractors} + _{Version} + $ \\(\\beta_{Semester} + \\beta_{Week} + \\beta_{Gender} + \\epsilon\\)\n\n\n5. Data Organization\nThe raw data for the project were collected over the course of three semesters (Fall 2020, Spring 2021, and Fall 2021) using the Qualtrics survey delivery platform. Data for the current analyses were drawn from three separate baseline survey data files, one for each semester. The three files were nearly identical and only semester-specific response options (e.g., professor, class time) were different. The measures were all taken before the intervention (pre-random assignment). Self-report survey measures, physics assessment measures, and demographics were all collected in the same survey.\n\n5.1 Data Cleansing and Tidying\n\nDownloading data files\n\nRaw RDS files were downloaded using the qualtRics package for R\nThese files were archived in a folder called ‘RDS’\n\n\n\nRun self-defined functions for cleaning and tidying\nThe following functions were written to standardize and streamline basic cleaning steps for all surveys. The basic operations of the cleaning and scoring functions are described below.\n\nRead.Raw.File(survey, semester): This function takes a survey name and semester (e.g., “Baseline”, “Fall20”) and performs the following steps:\n\nReads the raw RDS file\nRemoves unnecessary variables (e.g., empty Qualtrics-generated variables)\nAdds the following metadata variables by referencing a master spreadsheet (not all of these will be used in the current analysis):\n\nTimepoint: Experiment timepoint (Baseline)\nID: Defined above\nCondition: Experimental condition\nSemester (Cohort): Defined above\nWeek: Defined above\nWave: Week number (relative to the experiment that semester, e.g., Wave 1 = first round of students who participated in the study that semester)\n\nReferences a spreadsheet to recode variable values and rename variables\nRemoves all observations that do not match the names in the study master list (e.g., pilot or testing observations)\nRemoves all observations with survey progress less than 2% (false starts)\n\nScore.Physics(): This function takes the data frame generated from Read.Raw.File, collapses the two versions of the physics assessment (A and B), scores them based on the appropriate answer key, and then checks the total score against the one generated in Qualtrics (Qualtrics provides a total score, but not item-level accuracy).\n\n\n\nData auditing\n\nDuplicates (semantic violations):\n\n2 participants (YR37 and 5253) had multiple observations (1 partial and 1 complete)\nProgress for incomplete responses were 5% and 29% respectively. One was due to a laptop running out of batteries, the other cause was unknown.\nThe incomplete responses were removed and a note was added to their observations\n\nMissing values (coverage violations):\n\n2 additional participants (7647 and CQ94) completed 92% and 91% of the survey and had missing values near the end (demographics).\nResponses were retained, gender identity was imputed from screening survey data, notes were added to their observations\n\n\n\n\nDeidentify\n\nAfter running the cleaning and scoring functions and checking for anomalies, participant names were removed from the data.\n\n\n\n\n5.2 Data Architecture\n\nThe full baseline surveys for each semester were merged using bind_rows()\nThe baseline data frame was then split by measure type (self-report surveys, physics assessment, and demographics), each containing a set of ID and metadata variables to join by.\n\n\nSelf-report survey measures and demographics\n\nA psychological threat score (Psych.Threat.Diff) was calculated in the self-report survey dataframe by calculating a mean score for the resources and demands items separately, and then subtracting resources from demands such that greater scores indicate greater psychological threat.\nSelf-reported gender identity (gender) was selected from the demographics dataframe\n\n\n\nPhysics assessment measures\n\nThe physics assessment dataframe was pivoted long so that each item was a separate observation and each participant was represented across multiple rows\nQuestions were filtered to contain only categorization task items (Part 2, questions 2:6)\nFactor variables were added for Accuracy (based on Score) and Distractors\n\n\n\nResulting tidy-compliant table\nlibrary(tidyverse) |&gt; suppressMessages() \n\nPart2 &lt;- readRDS(\"../../DSPN_FinalProject.rds\") %&gt;% \n  # Remove centered and z-scored time variables because they are not needed  \n  select(-c(Time.z, Time.c))\n\nPart2 %&gt;% head \n\nA tibble: 6 × 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSemester\nWeek\nPhysAssessVersion\nQuestion\nResponse\nScore\nTime\nDistractors\nAccuracy\nPsych.Threat.Diff\nGender\n\n\n\n\n&lt;chr&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\nAL15\nFall20\n11\nVersion B\nQuestion 2\n2\n0\n76.853\nPresent\nIncorrect\n-1.533333\nMale\n\n\nAL15\nFall20\n11\nVersion B\nQuestion 3\n2\n1\n85.451\nAbsent\nCorrect\n-1.533333\nMale\n\n\nAL15\nFall20\n11\nVersion B\nQuestion 4\n1\n0\n115.285\nPresent\nIncorrect\n-1.533333\nMale\n\n\nAL15\nFall20\n11\nVersion B\nQuestion 5\n1\n0\n75.152\nPresent\nIncorrect\n-1.533333\nMale\n\n\nAL15\nFall20\n11\nVersion B\nQuestion 6\n1\n1\n131.814\nPresent\nCorrect\n-1.533333\nMale\n\n\nAF57\nFall20\n11\nVersion A\nQuestion 2\n1\n0\n76.357\nPresent\nIncorrect\n2.600000\nMale\n\n\n\n\n\n\n\n6. Analysis\nData Analysis Steps: 1. Load packages and define variables 1. Description of continuous variables 1. Data visualization 1. Mixed effects models 1. Bayes factor estimation\n\n6.1 Load Packages and Define Variables:\n# Load Packages\nlibrary(lme4) |&gt; suppressMessages()\nlibrary(sjPlot) |&gt; suppressMessages()\nlibrary(sjmisc) |&gt; suppressMessages()\nlibrary(sjlabelled) |&gt; suppressMessages()\nlibrary(ggpubr) |&gt; suppressMessages()\nlibrary(outliers) |&gt; suppressMessages()\nlibrary(IRdisplay) |&gt; suppressMessages()\nlibrary(moments) |&gt; suppressMessages()\n\n# Define Palettes\ncol_accuracy &lt;- c(\"indianred\", \"cadetblue3\")\ncol_distract &lt;- c(\"azure4\",\"darkorange\")\n\n\n6.2 Description of Continuous Variables\nFirst: Create summary table\n# Test for outliers\n############################\n# This function takes a variable vector and a number of iterations (default = 20) \n# and performs the grubbs test iteratively until the p value is greater than or equal to .05. \n# It then returns the number of outliers detected\noutlier.test &lt;- function(var, R=20){\n  outliers &lt;- as.numeric(c())\n  for(i in 1:R){\n    if(i == 1){dat = var} else{dat = out.rm}\n    grubbs &lt;- grubbs.test(dat)\n    if(grubbs$p.value &gt;= .05){\n      break\n    } else {\n      row.num &lt;- which.max(dat)\n      value &lt;- dat[row.num]\n      outliers &lt;- append(outliers, value)\n      out.rm &lt;- dat[-row.num]\n    }\n  }\n  return(outliers %&gt;% length)\n}\n\n# Summary table of continuous variables\n\n# Helper function for the table\nsummary.stats &lt;- function(df){\n  df %&gt;% \n    summarize(across(\n      everything(),\n      list(\n        min = min,\n        median = median,\n        mean = mean,\n        max = max,\n        skewness = skewness,\n        kurtosis = kurtosis,\n        # Shapiro-Wilk's test of normality\n        # If the test is significant, the distribution is non-normal\n        shapiro.p = ~ shapiro.test(.x)$p.value,\n        outliers = ~outlier.test(.x)\n      ),\n      .names = \"{.col}_{.fn}\"\n    )) %&gt;% \n    pivot_longer(\n      cols = everything(),\n      names_to = c(\"Variable\", \".value\"),\n      names_pattern = \"(.*)_(.*)\"\n    ) %&gt;% \n    mutate(across(min:shapiro.p, ~sprintf(\"%.2f\",.x)))\n}\n\n# Because the table is set up at the observation level, I'm gathering each participants\n# unique psychological threat score for the summary table. Otherwise each score would \n# be repeated 5 times (once for each question answered by 1 participant)\nthreat &lt;- Part2 %&gt;% group_by(ID) %&gt;% \n  summarise(Psych.Threat.Diff = unique(Psych.Threat.Diff))\n\nbind_rows(\n  Part2 %&gt;% select(Time) %&gt;% summary.stats(),\n  threat %&gt;% select(-ID) %&gt;% summary.stats()\n)\n\nA tibble: 2 × 9\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nmin\nmedian\nmean\nmax\nskewness\nkurtosis\nshapiro.p\noutliers\n\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\nTime\n8.40\n54.58\n59.65\n196.15\n0.94\n4.21\n0.00\n5\n\n\nPsych.Threat.Diff\n-2.40\n0.67\n0.70\n3.57\n0.03\n2.46\n0.47\n0\n\n\n\n\nNote: Psychological threat appears to be normally distributed, but solution time returned a significant result with the Shapiro Wilk’s test and five outliers were detected.\n\nNext: Visually inspect the distributions before making a decision about transformation.\noptions(repr.plot.width=9, repr.plot.height=7)\n\nggarrange(\n  ggdensity(\n    Part2$Time,\n    xlab = \"Solution Time in Seconds\",\n    title = \"Density Distribution of Time\",\n    fill = \"lightgray\"\n  ),\n  ggdensity(\n    threat$Psych.Threat.Diff,\n    xlab = \"Psychological Threat Score\",\n    title = \"Density Distribution of Psychological Threat\",\n    fill = \"lightgray\"\n  ),\n  ggqqplot(Part2$Time, title = \"QQ Plot of Solution Time\"),\n  ggqqplot(threat$Psych.Threat.Diff, title = \"QQ Plot of Psychological Threat\")\n) \n\n\n\npng\n\n\n\nNote: In line with the summary statistics, psychological threat appears to be normally distributed. Alternatively, the solution time variable has a positive skew. This makes sense because solution times are bounded at the lower end, but not at the higher end.\n\nNext: Because solution time is an outcome variable in the H2 model and normality is assumed, apply a log transformation and look at summary statistics again.\noptions(repr.plot.width=4, repr.plot.height=3)\n\nPart2 %&gt;% \n  mutate(\n    Time.log = log(Time),\n    # Centering the transformed solution time variable for moderation\n    Time.log.c = scale(Time.log, center = TRUE, scale = FALSE)\n  ) -&gt; Part2\n\n# Get summary stats on the new variable\nPart2 %&gt;% select(Time.log) %&gt;% summary.stats()\n\n# Visualize the new distribution\nggdensity(\n    Part2$Time.log,\n    xlab = \"Log Transformed Solution Time\",\n    fill = \"lightgray\"\n  )\n\nA tibble: 1 × 9\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nmin\nmedian\nmean\nmax\nskewness\nkurtosis\nshapiro.p\noutliers\n\n\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;int&gt;\n\n\nTime.log\n2.13\n4.00\n3.96\n5.28\n-0.52\n3.28\n0.00\n0\n\n\n\n\n\n\npng\n\n\n\nNote: The new variable now has a slight negative skew. However, the magnitude of the skewness is about 55% of what it was. Moreover, there are now no outlying observations which allows me to include all of the data.\n\n\n6.3 Visualize Relations Between Key Study Variables\nFirst: Look at accuracy by item and item type\noptions(repr.plot.width=11, repr.plot.height=7)\n\nPart2 %&gt;% \n  filter(Distractors == \"Present\") %&gt;% \n  group_by(PhysAssessVersion) %&gt;% \n  summarise(mean.accuracy = mean(Score)) -&gt; hline\n\nPart2 %&gt;%\n  group_by(Question, PhysAssessVersion) %&gt;%\n  summarise(.groups = \"keep\", Distractors = Distractors, Accuracy = mean(Score) %&gt;% round(2)) %&gt;%\n  ggbarplot(\n    x = \"Question\",\n    y = \"Accuracy\",\n    facet = \"PhysAssessVersion\",\n    add = \"mean\",\n    fill = \"Distractors\",\n    palette = col_distract,\n    size = .2,\n    alpha = .7,\n    ggtheme = theme_bw(base_size = 19),\n    xlab = FALSE,\n    title = \"Figure 2. Accuracy by Question and Distractors\\n\",\n    legend = \"right\"\n  )  + \n  rotate_x_text(angle = 45) +\n  scale_y_continuous(labels = scales::percent, limits = c(0, .8)) +\n  theme(panel.grid = element_blank(), title = element_text(face = \"bold\"), plot.title = element_text(hjust = .5)) +\n  geom_hline(data = hline, aes(yintercept = mean.accuracy), linetype = \"dashed\", color = \"chocolate\")\n\n\n\npng\n\n\n\nNote. It’s clear from this plot that, in line with prior work, there is an association between accuracy and surface feature distractors. It is also clear that there is one distractors-present question which seems to be an anomoly with respect to accuracy (Version A, Question 6).\n\nNext: Plot accuracy and time by distractors at the item level\noptions(repr.plot.width=16, repr.plot.height=7)\n\nggdensity(\n  Part2,\n  \"Time\",\n  \"..count..\",\n  fill = \"Accuracy\",\n  color = \"Accuracy\",\n  alpha = .4,\n  size = 1,\n  add = \"mean\",\n  facet.by = c(\"PhysAssessVersion\", \"Question\"),\n  palette = col_accuracy,\n  ggtheme = theme_bw(base_size = 19),\n  legend = \"right\"\n) + \n  rotate_x_text(angle = 45) + \n  labs(\n    title = \"Figure 3. Time by Accuracy and Question\\n\",\n    x = \"Time in Seconds (log transformed scale)\",\n    y = \"Response Count\"\n  ) +\n  scale_x_continuous(trans = \"log\", breaks = c(10,20,40,80,160)) +\n  theme(panel.grid = element_blank(), title = element_text(face = \"bold\"), plot.title = element_text(hjust = .5))\n\n\n\npng\n\n\n\nNote. This figure shows the response count (y-axis) of the incorrect (red) and correct (blue) responses across solution times (x-axis). The dotted lines represent the mean solution times for the incorrect and correct responses on a log transformed scale. As a reminder, question 3 in both test versions is the only distractors-absent question. As in the figure above, there’s a clear accuracy difference between the distractor/no-distractor questions. There is a smaller but noticable pattern of difference between the mean times. For 7/8 of the distractor questions (2, 4, 5, 6), the solution time for correct responses is greater than that for incorrect responses (exception = version A, question 4). Alternatively, on question 3, the solution time for the correct response is slightly faster.\n\nNext: Plot time by accuracy and distractors across questions\noptions(repr.plot.width=10, repr.plot.height=7)\n\nPart2 %&gt;%\n  ggboxplot(\n    \"Distractors\",\n    \"Time\",\n    color = \"Accuracy\",\n    fill = \"gray97\",\n    size = .6,\n    palette = col_accuracy,\n    add = \"jitter\",\n    add.params = list(size = 1, alpha = .7),\n    legend = \"right\",\n    legend.title = \"Accuracy\",\n    xlab = \"\\nSurface Feature Distractors\",\n    ylab = \"Time in Seconds (log transformed scale)\",\n    title = \"Figure 4. Time by Accuracy and Surface Feature Distractors\\n\",\n    ggtheme = theme_minimal(base_size = 19),\n  ) + \n  theme(\n    panel.grid.major.y = element_line(size = .2, color = \"gray90\"),\n    panel.grid.major.x = element_blank(),\n    axis.ticks = element_line(color = \"gray70\"),\n    title = element_text(face = \"bold\"),\n    plot.title = element_text(hjust = .5)) +\n  scale_y_continuous(trans = \"log\", limits = c(7, 200), breaks = c(12, 25, 50, 100, 200))\n\n\n\npng\n\n\n\nNote. In this figure, responses are collapsed across questions. You can see that there were more responses for distractors-present items (because there were more questions), and there were more incorrect responses for distractors-present items and more correct responses for distractors-absent items. An overall difference between the means is visible for both distractors-present and -absent items.\n\nNext: Look at time by distractors and self-reported psychological threat\noptions(repr.plot.width=10, repr.plot.height=7)\n\nPart2 %&gt;% \n  group_by(ID, Distractors) %&gt;% \n  summarise(\n    Psych.Threat = mean(Psych.Threat.Diff), \n    Time = mean(Time), .groups = \"keep\"\n  ) %&gt;% \n  ggscatter(\n    \"Psych.Threat\", \n    \"Time\", \n    color = \"Distractors\",\n    size = 2,\n    palette = col_distract,\n    alpha = .7,\n    ggtheme = theme_minimal(base_size = 19),\n    title = \"Figure 5. Time by Distractors and Psychological Threat\\n\",\n    ylab = \"Time in Seconds (log transformed scale)\",\n    xlab = \"Psychological Threat\",\n    legend = \"right\"\n  ) + \n  scale_y_continuous(trans = \"log\", breaks = c(10, 20, 40, 80, 160)) +\n  scale_x_continuous(breaks = seq(-3, 4, by = 1)) +\n  geom_smooth(\n    aes(color = Distractors), method = \"lm\", formula = 'y~x', size = 1, se = FALSE) +\n  theme(panel.grid.major = element_line(size = .2), \n        title = element_text(face = \"bold\"), plot.title = element_text(hjust = .5))\n\n\n\npng\n\n\n\nNote. This figure represents the relationship described by hypothesis 2. It appears unlikely that there is any association between psychological threat, distractors, and solution time.\n\n\n6.4 Mixed Effects Models\n\nHypothesis 1\nH1: There will be a significant interaction between item type (distractors or not), solution time, and accuracy. Specifically, we expect that when distractors are present, increases in solution time will be associated with higher likelihood of solving the problem correctly\n\\(Y_{Accuracy} = (\\beta_0 + {b_{ID,0}}_{id} + {b_{QUESTION,0}}_{question}) + \\beta_{Distractors} * \\beta_{Time} + \\beta_{Version} + \\beta_{Semester} + \\beta_{Week} + \\beta_{Gender} + \\epsilon\\)\nfit.h1.fun &lt;- function(data){\n    glmer(\n      Accuracy ~ \n        Distractors*Time.log.c + \n        (1|ID) + \n        (1|Question) +\n        PhysAssessVersion + \n        Semester + \n        Week +\n        Gender, \n      data = data, \n      family = \"binomial\", \n      glmerControl(optimizer = \"bobyqa\")\n    )  %&gt;% return\n}\n\nh1.fit &lt;- fit.h1.fun(Part2)\n\ntab_model(\n    h1.fit,  \n    title = \"Table 1. Results of Logististic Mixed Effects Model Testing Hypothesis 1\",\n    CSS = list(css.firsttablecol = 'width: 200px'),\n    file = \"h1_summary.html\"\n)\n\ndisplay_html(file=\"h1_summary.html\")\n\nTable 1. Results of Logististic Mixed Effects Model Testing Hypothesis 1\n\n\n\n\n\n\n\nAccuracy\n\n\n\n\nPredictors\nOdds Ratios\n\n\n(Intercept)\n2.98\n\n\nDistractors [Present]\n0.18\n\n\nTime log c\n0.69\n\n\nPhysAssessVersion\n[Version B]\n0.68\n\n\nSemester [Fall21]\n0.77\n\n\nSemester [Spring21]\n0.72\n\n\nWeek\n1.00\n\n\nGender [Male]\n1.09\n\n\nDistractors [Present] *\nTime log c\n2.55\n\n\nRandom Effects\n\n\n\nσ2\n3.29\n\n\nτ00 ID\n0.12\n\n\nτ00 Question\n0.12\n\n\nICC\n0.07\n\n\nN ID\n149\n\n\nN Question\n5\n\n\nObservations\n745\n\n\nMarginal R2 / Conditional R2\n0.150 / 0.208\n\n\n\n\n\n\nH1 Results\n\n\nThere is a significant main effect of distractors on accuracy\n\nThere is a main effect of test version, as could be seen in the visualizations above (Version A, Question 6)\n\nThere is a significant interaction between surface feature distractors and solution time on accuracy\n\n\n\nNext: Calculate the prediction accuracy of the model, visualize the interaction with model predictions, and do a simple slopes analysis.\nh1.df &lt;- Part2 %&gt;% \n  mutate(\n    prob_correct = predict(h1.fit, type = \"response\"),\n    pred_score = if_else(prob_correct &gt; .5, 1, 0)\n  )\n\nlibrary(caret) |&gt; suppressMessages()\n\ncm &lt;- h1.df %&gt;% \n  transmute(\n    Actual = Accuracy,\n    Predicted = if_else(pred_score == 1, \"Correct\", \"Incorrect\") %&gt;% \n      factor(levels = c(\"Incorrect\", \"Correct\"))\n  )\n\nfourfoldplot(table(cm))\n\nprediction.acc &lt;- mean(h1.df$Score == h1.df$pred_score) %&gt;% round(4)\n\npaste0(\"The prediction accuracy of the H1 model is \", (prediction.acc *100), \"%\") %&gt;% print\n\noptions(repr.plot.width=15, repr.plot.height=7)\n\np1 &lt;- plot_model(\n  h1.fit,\n  type = \"pred\",\n  terms = c(\"Time.log.c[all]\", \"Distractors\"),\n  show.values = TRUE,\n  colors = col_distract\n) +\n  theme_minimal(base_size = 19) +\n  labs(x = \"Centered Log of the Solution Time\",\n       title = \"Interaction Plot Using sjPlot\",\n       y = \"Predicted Probability of Answering Correctly\") +\n  theme(plot.title = element_text(face = \"bold\", hjust = .5)) \n\np2 &lt;- h1.df %&gt;% \n  ggscatter(\n    \"Time\",\n    \"prob_correct\",\n    color = \"Distractors\",\n    alpha = .7,\n    palette = col_distract,\n    ggtheme = theme_minimal(base_size=19),\n    title = \"Predicted Points Using predict()\",\n    ylab = \"Predicted Probability of Answering Correctly\",\n    xlab = \"Solution Time in Seconds (log transformed scale)\"\n  ) + \n  theme(plot.title = element_text(face = \"bold\", hjust = .5)) +\n  scale_x_continuous(trans = \"log\", breaks = c(12, 25, 50, 100, 200)) +\n  scale_y_continuous(limits = c(0,1), breaks = c(.25, .5, .75, 1), labels = scales::percent)\n\nggarrange(p1, p2, common.legend = TRUE)\n[1] \"The prediction accuracy of the H1 model is 71.41%\"\n\n\n\npng\n\n\n\n\n\npng\n\n\nlibrary(interactions)\n\nsim_slopes(h1.fit, pred = Time.log.c, modx = Distractors, johnson_neyman = FALSE) \n[1m[4mSIMPLE SLOPES ANALYSIS[24m[22m \n\n[3mSlope of Time.log.c when Distractors = Present: \n\n[23m  Est.   S.E.   z val.      p\n------ ------ -------- ------\n  0.57   0.18     3.14   0.00\n\n[3mSlope of Time.log.c when Distractors = Absent: \n\n[23m   Est.   S.E.   z val.      p\n------- ------ -------- ------\n  -0.37   0.39    -0.94   0.35\n\nNote. The prediction accuracy of the model on the training data is about 71%. As you can see from the visualizations and the simple slopes analysis, when distractors are present the probability of answering correctly increases significantly with time spent. Alternatively, the probability of answering correctly decreases slightly (although not significantly) when distractors are absent.\n\nNext: Fit the model for hypothesis 2\n\n\n\nHypothesis 2\nH2: Psychological threat, measured using self-report survey, will be negatively associated with time spent on the problem categorization task items when distractors are present.\n\\(Y_{Time} = (\\beta_0 + {b_{ID,0}}_{id} + {b_{QUESTION,0}}_{question}) + \\beta_{Distractors} * \\beta_{Psych.Threat.Diff} + \\beta_{Version} + \\beta_{Semester} + \\beta_{Week} + \\beta_{Gender} + \\epsilon\\)\nfit.h2.fun &lt;- function(data){\n    lmer(\n      Time.log.c ~ \n        Distractors*Psych.Threat.Diff + \n        (1|ID) + \n        (1|Question) +\n        Semester + \n        PhysAssessVersion + \n        Week +\n        Gender, \n      data = data\n    ) %&gt;% return()\n}\n\nh2.fit &lt;- fit.h2.fun(Part2)\n\ntab_model(\n    h2.fit, \n    title = \"Table 2. Results of Linear Mixed Effects Model Testing Hypothesis 2\",\n    CSS = list(css.firsttablecol = 'width: 200px'),\n    file = \"h2_summary.html\"\n)\ndisplay_html(file=\"h2_summary.html\")\n\nTable 2. Results of Linear Mixed Effects Model Testing Hypothesis 2\n\n\n\n\n\n\n\nTime.log.c\n\n\n\n\nPredictors\nEstimates\n\n\n(Intercept)\n0.53\n\n\nDistractors [Present]\n0.01\n\n\nPsych Threat Diff\n-0.01\n\n\nSemester [Fall21]\n-0.25\n\n\nSemester [Spring21]\n-0.39\n\n\nPhysAssessVersion\n[Version B]\n0.05\n\n\nWeek\n-0.04\n\n\nGender [Male]\n-0.00\n\n\nDistractors [Present] *\nPsych Threat Diff\n-0.00\n\n\nRandom Effects\n\n\n\nσ2\n0.13\n\n\nτ00 ID\n0.16\n\n\nτ00 Question\n0.01\n\n\nICC\n0.58\n\n\nN ID\n149\n\n\nN Question\n5\n\n\nObservations\n745\n\n\nMarginal R2 / Conditional R2\n0.019 / 0.584\n\n\n\n\n\n\nH2 Results\n\n\nIn line with Figure 5 above, there were no main effects of distractors or pychological threat on solution time and no interaction between psychological threat and distractors on solution time.\n\n\n\n\n\n6.5 Bayes Factor Estimation\n\nHypothesis 1\nH1: There will be a significant interaction between item type (distractors or not), solution time, and accuracy. Specifically, we expect that when distractors are present, increases in solution time will be associated with higher likelihood of solving the problem correctly\n\nI calculated a bayes factor to quantify the evidence in favor of the full model (with the interaction term) over the null model (without the interaction) using the brms package because the BayesFactor package can only handle linear mixed effects models. This analysis was informed by a tutorial written by Jonas Kristoffer Lindeløv.\n\nFirst: Fit the models using brms\nlibrary(brms) |&gt; suppressMessages()\n\nfull_brms &lt;- brm(\n  Accuracy ~ \n    Distractors*Time.log.c + \n    (1|ID) + \n    (1|Question) +\n    PhysAssessVersion + \n    Semester + \n    Week +\n    Gender, \n  data = Part2, \n  iter = 10000,\n  chains = 4,\n  seed = 123,\n  family = bernoulli(link = \"logit\"),\n  save_pars = save_pars(all = TRUE),\n  control = list(adapt_delta = .99)\n)\n\nnull_brms &lt;- update(full_brms, formula = ~.-Distractors:Time.log.c)\nCompiling Stan program...\n\nrecompiling to avoid crashing R session\n\nStart sampling\n\n\n\n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.94 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 26.7945 seconds (Warm-up)\nChain 1:                26.542 seconds (Sampling)\nChain 1:                53.3365 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.5 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 25.7026 seconds (Warm-up)\nChain 2:                27.6264 seconds (Sampling)\nChain 2:                53.3291 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5.4e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.54 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 25.7891 seconds (Warm-up)\nChain 3:                28.3744 seconds (Sampling)\nChain 3:                54.1635 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 25.8614 seconds (Warm-up)\nChain 4:                28.3992 seconds (Sampling)\nChain 4:                54.2605 seconds (Total)\nChain 4: \n\n\nStart sampling\n\n\n\n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 6.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.61 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 1: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 1: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 1: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 1: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 1: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 1: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 24.8325 seconds (Warm-up)\nChain 1:                14.6516 seconds (Sampling)\nChain 1:                39.4841 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5.9e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.59 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 2: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 2: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 2: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 2: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 2: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 2: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 28.9034 seconds (Warm-up)\nChain 2:                57.1974 seconds (Sampling)\nChain 2:                86.1009 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5.7e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.57 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 3: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 3: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 3: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 3: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 3: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 3: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 3: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 3: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 3: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 3: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 3: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 27.3007 seconds (Warm-up)\nChain 3:                29.0078 seconds (Sampling)\nChain 3:                56.3085 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'b0deb918f33676f323a2956cbb109561' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 5.7e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.57 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 4: Iteration: 1000 / 10000 [ 10%]  (Warmup)\nChain 4: Iteration: 2000 / 10000 [ 20%]  (Warmup)\nChain 4: Iteration: 3000 / 10000 [ 30%]  (Warmup)\nChain 4: Iteration: 4000 / 10000 [ 40%]  (Warmup)\nChain 4: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 4: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 4: Iteration: 6000 / 10000 [ 60%]  (Sampling)\nChain 4: Iteration: 7000 / 10000 [ 70%]  (Sampling)\nChain 4: Iteration: 8000 / 10000 [ 80%]  (Sampling)\nChain 4: Iteration: 9000 / 10000 [ 90%]  (Sampling)\nChain 4: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 26.7232 seconds (Warm-up)\nChain 4:                28.6709 seconds (Sampling)\nChain 4:                55.3942 seconds (Total)\nChain 4: \n# Bayesian model summaries\nfull_brms %&gt;% summary\nnull_brms %&gt;% summary\n Family: bernoulli \n  Links: mu = logit \nFormula: Accuracy ~ Distractors * Time.log.c + (1 | ID) + (1 | Question) + PhysAssessVersion + Semester + Week + Gender \n   Data: Part2 (Number of observations: 745) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nGroup-Level Effects: \n~ID (Number of levels: 149) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.45      0.20     0.05     0.81 1.00     4050     5794\n\n~Question (Number of levels: 5) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.83      0.56     0.26     2.31 1.00     6484     9148\n\nPopulation-Level Effects: \n                              Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                         1.15      1.48    -1.76     4.07 1.00\nDistractorsPresent               -1.79      1.13    -4.11     0.52 1.00\nTime.log.c                       -0.38      0.41    -1.19     0.41 1.00\nPhysAssessVersionVersionB        -0.40      0.19    -0.78    -0.04 1.00\nSemesterFall21                   -0.27      0.49    -1.25     0.69 1.00\nSemesterSpring21                 -0.34      0.57    -1.47     0.76 1.00\nWeek                              0.00      0.09    -0.17     0.18 1.00\nGenderMale                        0.09      0.20    -0.29     0.48 1.00\nDistractorsPresent:Time.log.c     0.98      0.44     0.14     1.86 1.00\n                              Bulk_ESS Tail_ESS\nIntercept                        12756    10319\nDistractorsPresent               12427     8630\nTime.log.c                       23847    14661\nPhysAssessVersionVersionB        31025    15419\nSemesterFall21                   14404    14474\nSemesterSpring21                 15062    14050\nWeek                             14302    13945\nGenderMale                       28176    14782\nDistractorsPresent:Time.log.c    24123    15318\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Accuracy ~ Distractors + Time.log.c + (1 | ID) + (1 | Question) + PhysAssessVersion + Semester + Week + Gender \n   Data: Part2 (Number of observations: 745) \n  Draws: 4 chains, each with iter = 10000; warmup = 5000; thin = 1;\n         total post-warmup draws = 20000\n\nGroup-Level Effects: \n~ID (Number of levels: 149) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.45      0.19     0.05     0.81 1.00     3454     4164\n\n~Question (Number of levels: 5) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.82      0.55     0.25     2.28 1.00     5529     7504\n\nPopulation-Level Effects: \n                          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                     1.05      1.47    -1.81     3.90 1.00    10430\nDistractorsPresent           -1.79      1.11    -4.04     0.46 1.00     9976\nTime.log.c                    0.43      0.17     0.10     0.77 1.00    30492\nPhysAssessVersionVersionB    -0.41      0.19    -0.78    -0.05 1.00    28438\nSemesterFall21               -0.23      0.49    -1.19     0.72 1.00    11968\nSemesterSpring21             -0.31      0.56    -1.40     0.80 1.00    12498\nWeek                          0.01      0.09    -0.16     0.18 1.00    11834\nGenderMale                    0.11      0.19    -0.27     0.49 1.00    25693\n                          Tail_ESS\nIntercept                    10270\nDistractorsPresent            8438\nTime.log.c                   15397\nPhysAssessVersionVersionB    15749\nSemesterFall21               13665\nSemesterSpring21             13772\nWeek                         13733\nGenderMale                   16261\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNext: Calculate Bayes factor\nNote. I calculated the bayes factor in favor of the interaction against the null model with brms::bayes_factor, which uses a bridge sampling algorithm. Because the calculation produces slightly different results each time it is run, it is recommended that the function is called multiple times to ensure the stability of the results. Therefore, ran the calculation 100 times and plotted the distribution of bayes factor results.\nget.bf.values &lt;- function(R, model1, model2) {\n  values &lt;- rep(NA, R)\n  for (i in 1:R) {\n    this.bf &lt;- bayes_factor(model1, model2, silent = TRUE) # The output was insanely long so I silenced it\n    values[i] &lt;- this.bf$bf\n  }\n  values %&gt;% return\n}\n\nbf_values &lt;- get.bf.values(100, full_brms, null_brms)\nbf_ci &lt;- bf_values %&gt;% quantile(probs = c(.025, .975)) %&gt;% round(2)\nbf_mean &lt;- bf_values %&gt;% mean%&gt;% round(2)\n\noptions(repr.plot.width=7, repr.plot.height=5) \n\ngghistogram(\n  bf_values,\n  bins = 11,\n  fill = \"lightgray\",\n  ggtheme = theme_bw(base_size = 15),\n  xlab = \"Estimated Bayes factors in favor of full_brms over null_brms\",\n  ylab = \"Count\",\n  title = \"Estimated Bayes Factors with Mean and 95% CI\"\n  ) +\n  geom_vline(xintercept = bf_ci, linetype = \"dashed\") +\n  geom_vline(xintercept = bf_mean, linetype = \"dashed\", color = \"red\") +\n  scale_x_continuous(breaks = seq(13, 16, by = .5))\n\n\nlibrary(effectsize)\n\n# Interpret Bayes factor\n\npaste0(\n  \"The mean Bayes factor estimate was \",\n  bf_mean,\n  \", 95% CI[\",\n  bf_ci[1],\n  \", \",\n  bf_ci[2],\n  \"]. This indicates \",\n  interpret_bf(bf_mean),\n  \" the full model.\"\n)\n‘The mean Bayes factor estimate was 14.57, 95% CI[13.56, 15.66]. This indicates strong evidence in favour of the full model.’\n\n\n\npng\n\n\n\n\n\nBayes Factor Estimation Results for H1\n\n\nThe Bayes factor estimates using bridge sampling were stable, with 95% of the values falling roughly between 13.6 and 15.7. This indicates strong evidence in favor of the interaction model compared to the null (Jeffreys, 1961).\n\n\n\n\n\nHypothesis 2\nH2: Psychological threat, measured using self-report survey, will be negatively associated with time spent on the problem categorization task items when distractors are present.\nNote: Because I found no significant results with my H2 model, I wanted to look at the evidence in favor of the full model relative to the evidence in favor of the intercept-only model (mean solution time as the only predictor) as well as the no-interaction model. Because H2 was tested with a mixed linear model, I used the BayesFactor package.\n\nNext: Fit the full model (with interaction) and the null model (no interaction) and plot them against the intercept-only model. Then compare the full and the null model against each other and interpret the results.\nlibrary(BayesFactor) |&gt; suppressMessages()\n# BayesFactor wants the random variables as factors \n# and data as data frame and not tibble\n\nPart2.1 &lt;- Part2 %&gt;% \n  mutate(across(c(ID, Question), factor)) %&gt;% \n  as.data.frame()\n\nh2.bf.full &lt;- lmBF(\n  Time.log.c ~ \n    Distractors +\n    Psych.Threat.Diff +\n    Distractors*Psych.Threat.Diff + \n    Semester + \n    PhysAssessVersion + \n    Week +\n    Gender +\n    ID +\n    Question,  \n  data = Part2.1,\n  whichRandom = c('ID', 'Question')\n)\n\nh2.bf.null &lt;- lmBF(\n  Time.log.c ~ \n    Distractors + \n    Psych.Threat.Diff + \n    Semester + \n    PhysAssessVersion + \n    Week +\n    Gender +\n    ID +\n    Question,  \n  data = Part2.1,\n  whichRandom = c('ID', 'Question')\n)\n\nh2.bf.all &lt;- c(h2.bf.full, h2.bf.null)\n\noptions(repr.plot.width=15, repr.plot.height=5) \n\nplot(h2.bf.all)\n\ncompare_bf &lt;- h2.bf.full/h2.bf.null\ncompare_bf\nBayes factor analysis\n--------------\n[1] Distractors + Psych.Threat.Diff + Distractors * Psych.Threat.Diff + Semester + PhysAssessVersion + Week + Gender + ID + Question : 0.1426753 ±4.45%\n\nAgainst denominator:\n  Time.log.c ~ Distractors + Psych.Threat.Diff + Semester + PhysAssessVersion + Week + Gender + ID + Question \n---\nBayes factor type: BFlinearModel, JZS\n\n\n\npng\n\n\n# Interpret Bayes factor\ncompare_bf %&gt;% as.vector %&gt;% as.numeric %&gt;% interpret_bf %&gt;%\n  paste(., \"the full model compared to the no-interaction model\")\n‘moderate evidence against the full model compared to the no-interaction model’\n\n\n\nBayes Factor Estimation Results for H2\n\n\nThere was extreme evidence for the full (interaction) and the null (no interaction) compared to the intercept-only model, but there was only a moderate difference between them, with the null model doing slightly better than the full model.\n\nThis makes sense with respect to the marginal and conditional \\(R^2\\) values reported in the model results. The marginal \\(R^2\\) represents the variablility explained by the fixed effects and the conditional \\(R^2\\) represents the variability explained by the random effects. As a reminder, these were the results for the H2 linear mixed model: marginal \\(R^2\\) = 0.019, conditional \\(R^2\\) = .584. This means that the random intercepts (participant means and question means) do a great job at predicting solution time, but the fixed effects do not add much above and beyond that. Because the models both include the random effect of ID (individual differences between participants), they are much better than the intercept (grand mean) at predicting solution time, but there is little difference between them.\n\n\n\n\n\n\n7. Conclusions\nIn the current study we tested two hypotheses in a sample of undergraduate physics students reporting psychological threat. Solution time and accuracy were measured during a problem categorization task in which items varied on whether there were surface feature distractors present in the incorrect response option. First, we tested whether there was an interaction between solution time and distractors on accuracy. In line with prior work (e.g., Hardiman et al., 1989), we found that surface feature distractors reduced the likelihood of answering correctly. We also found evidence in favor of an interaction between distractors and solution time. When distractors were present, the probability of answering correctly increased with time. There was no relationship between time and accuracy when distractors were absent. Next, we tested whether there was an interaction between psychological threat and surface feature distractors on solution time. We had predicted that more threatened students would be oriented toward avoidance and thus be more likely to answer quickly when a superficially similar response was available. We found no evidence supporting either the main effects of, or interaction between psychological threat and distractors on solution time. The model as a whole was much better at predicting solution time compared to the mean, but this was due to unexplained individual differences between participants and questions.\nThis study has some important limitations. There were only five items on the categorization task which were unbalanced with regard to item type (distractors vs. no distractors). Future work should test this hypothesis with more total items and an equal number of distractor and no-distractor items. Additionally, solution times were measured online using Qualtrics, which is not intended to provide precise reaction time data, and our analysis revealed that the test versions were not completely equivalent as intended. Another limitation of this study is that our sample was selected based on initial psychological threat scores greater than zero during the screening survey, and therefore it is unknown whether results will generalize to a broader population of physics students. Selection bias may also explain why there was no effect of psychological threat found in the test of hypothesis 2.\nWe build on prior work by showing evidence that the availability of a superficially similar response option is associated with solution time as well as accuracy when categorizing physics problems. The greater accuracy on distractors-absent items suggests that novice categorization decisions are not entirely explained by a lack of knowledge or exclusive reliance on surface features. Rather, the problem may be that novices are less likely to recognize and inhibit heuristic responses, and that inhibiting these respoonses takes time. Understanding this process can help inform physics instruction."
  },
  {
    "objectID": "projects/interpreting_experiments/index.html",
    "href": "projects/interpreting_experiments/index.html",
    "title": "Interpreting Experiments",
    "section": "",
    "text": "About\nThis interactive lesson was designed as part of a self-directed final project for E-Learning Design Principles and Methods, a graduate-level course taken during Fall 2023 at Carnegie Melon University with Dr. Ken Koedinger.\nThe activity was designed using the Qualtrics survey platform using custom-written HTML, CSS, and JavaScript. Click here to preview the lesson and provide some feedback!"
  },
  {
    "objectID": "projects/shiny_app/index.html",
    "href": "projects/shiny_app/index.html",
    "title": "College ScoreCard Explorer",
    "section": "",
    "text": "Preview the app\nThis app allows you to extensively explore a large data set containing information about institutions of higher learning in the U.S. and outlying areas, spanning nearly a quarter century.\nAbout the Data With this interface, you will have access to the institution-level data files for all academic years from 2011-12 through 2020-21 (data from 1996-97 to 2010-11 are not included to space limitations). The data are available for download from the U.S. Department of Education. Visit their website for more information or to download the files yourself: https://collegescorecard.ed.gov/data/"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications"
  }
]